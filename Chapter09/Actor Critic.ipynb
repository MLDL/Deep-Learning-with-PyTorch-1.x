{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number 10\tLast reward: 24.00\tAverage reward: 21.77\n",
      "Episode number 20\tLast reward: 200.00\tAverage reward: 88.68\n",
      "Episode number 30\tLast reward: 100.00\tAverage reward: 101.79\n",
      "Episode number 40\tLast reward: 134.00\tAverage reward: 115.30\n",
      "Episode number 50\tLast reward: 170.00\tAverage reward: 131.04\n",
      "Episode number 60\tLast reward: 140.00\tAverage reward: 130.99\n",
      "Episode number 70\tLast reward: 200.00\tAverage reward: 145.82\n",
      "Episode number 80\tLast reward: 173.00\tAverage reward: 162.88\n",
      "Episode number 90\tLast reward: 141.00\tAverage reward: 169.09\n",
      "Episode number 100\tLast reward: 174.00\tAverage reward: 164.69\n",
      "Episode number 110\tLast reward: 200.00\tAverage reward: 176.47\n",
      "Episode number 120\tLast reward: 183.00\tAverage reward: 185.06\n",
      "Episode number 130\tLast reward: 200.00\tAverage reward: 182.33\n",
      "Episode number 140\tLast reward: 187.00\tAverage reward: 187.80\n",
      "Episode number 150\tLast reward: 191.00\tAverage reward: 186.57\n",
      "Episode number 160\tLast reward: 192.00\tAverage reward: 181.48\n",
      "Episode number 170\tLast reward: 189.00\tAverage reward: 178.21\n",
      "Episode number 180\tLast reward: 200.00\tAverage reward: 185.34\n",
      "Episode number 190\tLast reward: 200.00\tAverage reward: 191.22\n",
      "Episode number 200\tLast reward: 200.00\tAverage reward: 194.74\n",
      "Solved! Running reward is 195.00753109917716 and the last episode runs to 200 time steps!\n"
     ]
    }
   ],
   "source": [
    "HistoricalAction = namedtuple('HistoricalAction', ['log_prob', 'value'])\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.linear = nn.Linear(4, 128)\n",
    "        self.head_action = nn.Linear(128, 2)\n",
    "        self.head_value = nn.Linear(128, 1)\n",
    "\n",
    "        self.historical_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear(x))\n",
    "        action_scores = self.head_action(x)\n",
    "        state_values = self.head_value(x)\n",
    "        return F.softmax(action_scores, dim=-1), state_values\n",
    "\n",
    "\n",
    "model = ActorCritic()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def choose_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.historical_actions.append(HistoricalAction(m.log_prob(action), state_value))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def end_episode():\n",
    "    R = 0\n",
    "    historical_actions = model.historical_actions\n",
    "    losses_policy = []\n",
    "    losses_value = []\n",
    "    returns = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for (log_prob, value), R in zip(historical_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        losses_policy.append(-log_prob * advantage)\n",
    "        losses_value.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(losses_policy).sum() + torch.stack(losses_value).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.historical_actions[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, ep_reward = environment.reset(), 0\n",
    "        for t in range(1, 10000):  \n",
    "            action = choose_action(state)\n",
    "            state, reward, done, _ = environment.step(action)\n",
    "            if render:\n",
    "                environment.render()\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        end_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode number {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > environment.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "seed = 100\n",
    "gamma = 0.99\n",
    "render = 'store_true'\n",
    "log_interval = 10\n",
    "\n",
    "environment = gym.make('CartPole-v0')\n",
    "environment.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
