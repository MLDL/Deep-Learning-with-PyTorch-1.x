{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(1); torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for model\n",
    "l_rate = 0.01\n",
    "gamma_value = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class PolicyGradient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "        \n",
    "        # Define the action space and state space\n",
    "        self.action_space = env.action_space.n\n",
    "        self.state_space = env.observation_space.shape[0]\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma_value = gamma_value\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.history_policy = Variable(torch.Tensor()) \n",
    "        self.reward_episode = []\n",
    "        \n",
    "        # Overall reward and loss history\n",
    "        self.history_reward = []\n",
    "        self.history_loss = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)\n",
    "    \n",
    "policy = PolicyGradient()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number 0, Last length:    19, Average length: 10.09\n",
      "Episode number 50, Last length:    13, Average length: 12.81\n",
      "Episode number 100, Last length:    10, Average length: 11.77\n",
      "Episode number 150, Last length:    10, Average length: 11.42\n",
      "Episode number 200, Last length:     8, Average length: 10.69\n",
      "Episode number 250, Last length:    13, Average length: 11.32\n",
      "Episode number 300, Last length:    11, Average length: 11.07\n",
      "Episode number 350, Last length:     9, Average length: 10.76\n",
      "Episode number 400, Last length:     8, Average length: 10.34\n",
      "Episode number 450, Last length:     8, Average length: 10.06\n",
      "Episode number 500, Last length:    10, Average length: 9.68\n",
      "Episode number 550, Last length:     9, Average length: 9.57\n",
      "Episode number 600, Last length:     9, Average length: 9.31\n",
      "Episode number 650, Last length:     9, Average length: 9.28\n",
      "Episode number 700, Last length:     8, Average length: 9.10\n",
      "Episode number 750, Last length:     9, Average length: 9.05\n",
      "Episode number 800, Last length:    10, Average length: 9.05\n",
      "Episode number 850, Last length:     7, Average length: 8.89\n",
      "Episode number 900, Last length:     8, Average length: 8.89\n",
      "Episode number 950, Last length:     9, Average length: 8.87\n"
     ]
    }
   ],
   "source": [
    "def choose_action(state):\n",
    "    # Run the policy model and choose an action based on the probabilities in state\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    state = policy(Variable(state))\n",
    "    c = Categorical(state)\n",
    "    action = c.sample()   \n",
    "    if policy.history_policy.dim() != 0:\n",
    "        try:\n",
    "            policy.history_policy = torch.cat([policy.history_policy, c.log_prob(action)])\n",
    "        except:\n",
    "            policy.history_policy = (c.log_prob(action))\n",
    "    else:\n",
    "        policy.history_policy = (c.log_prob(action))\n",
    "    return action\n",
    "\n",
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma_value * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    x = np.finfo(np.float32).eps\n",
    "    x = np.array(x)\n",
    "    x = torch.from_numpy(x)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + x)\n",
    "\n",
    "    # Calculate the loss loss\n",
    "    loss = (torch.sum(torch.mul(policy.history_policy, Variable(rewards)).mul(-1), -1))\n",
    "    \n",
    "    # Update the weights of the network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.history_loss.append(loss.data[0])\n",
    "    policy.history_reward.append(np.sum(policy.reward_episode))\n",
    "    policy.history_policy = Variable(torch.Tensor())\n",
    "    policy.reward_episode= []\n",
    "    \n",
    "def main_function(episodes):\n",
    "    running_total_reward = 10\n",
    "    for e in range(episodes):\n",
    "        # Reset the environment and record the starting state\n",
    "        state = env.reset() \n",
    "        done = False       \n",
    "    \n",
    "        for time in range(1000):\n",
    "            action = choose_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action.data.item())\n",
    "\n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_total_reward = (running_total_reward * 0.99) + (time * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            print('Episode number {}, Last length: {:5d}, Average length: {:.2f}'.format(e, time, running_total_reward))\n",
    "\n",
    "        if running_total_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_total_reward, time))\n",
    "            break\n",
    "\n",
    "episodes = 1000\n",
    "main_function(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
