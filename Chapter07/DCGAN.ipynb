{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.utils as vutils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "img_size = 64\n",
    "batch_size=64\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "niter= 25\n",
    "outf= 'output'\n",
    "\n",
    "dataset = datasets.CIFAR10( root = 'data',download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Resize(img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size,\n",
    "                                         shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of latnet vector\n",
    "nz = 100\n",
    "# Filter size of generator\n",
    "ngf = 64\n",
    "# Filter size of discriminator\n",
    "ndf = 64\n",
    "# Output image channels\n",
    "nc = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_inititialisation(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_net_generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _net_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_net_generator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "net_generator = _net_generator()\n",
    "net_generator.apply(weights_inititialisation)\n",
    "print(net_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_net_discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class _net_discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_net_discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "net_discriminator = _net_discriminator()\n",
    "net_discriminator.apply(weights_inititialisation)\n",
    "print(net_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "input = torch.FloatTensor(batch_size, 3, img_size, img_size)\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "label = torch.FloatTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net_discriminator.cuda()\n",
    "    net_generator.cuda()\n",
    "    criterion.cuda()\n",
    "    input, label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "optimizer_discriminator = optim.Adam(net_discriminator.parameters(), lr, betas=(beta1, 0.95))\n",
    "optimizer_generator = optim.Adam(net_generator.parameters(), lr, betas=(beta1, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782] Loss_Discriminator: 1.5009 Loss_Generator: 5.5792 D(x): 0.4640 D(G(z)): 0.4075 / 0.0047\n",
      "[0/25][1/782] Loss_Discriminator: 0.9016 Loss_Generator: 6.4903 D(x): 0.7805 D(G(z)): 0.4035 / 0.0020\n",
      "[0/25][2/782] Loss_Discriminator: 0.8814 Loss_Generator: 6.0653 D(x): 0.7306 D(G(z)): 0.3064 / 0.0030\n",
      "[0/25][3/782] Loss_Discriminator: 1.1415 Loss_Generator: 6.0359 D(x): 0.6943 D(G(z)): 0.3669 / 0.0032\n",
      "[0/25][4/782] Loss_Discriminator: 1.1851 Loss_Generator: 7.0107 D(x): 0.7264 D(G(z)): 0.4358 / 0.0013\n",
      "[0/25][5/782] Loss_Discriminator: 1.0506 Loss_Generator: 7.8329 D(x): 0.7478 D(G(z)): 0.4270 / 0.0006\n",
      "[0/25][6/782] Loss_Discriminator: 0.9600 Loss_Generator: 7.1500 D(x): 0.6811 D(G(z)): 0.2328 / 0.0013\n",
      "[0/25][7/782] Loss_Discriminator: 0.9689 Loss_Generator: 7.8842 D(x): 0.6859 D(G(z)): 0.3404 / 0.0006\n",
      "[0/25][8/782] Loss_Discriminator: 0.5686 Loss_Generator: 8.9665 D(x): 0.8327 D(G(z)): 0.2671 / 0.0002\n",
      "[0/25][9/782] Loss_Discriminator: 0.6097 Loss_Generator: 7.3556 D(x): 0.7349 D(G(z)): 0.1392 / 0.0011\n",
      "[0/25][10/782] Loss_Discriminator: 1.0898 Loss_Generator: 11.5623 D(x): 0.9078 D(G(z)): 0.5772 / 0.0000\n",
      "[0/25][11/782] Loss_Discriminator: 0.6181 Loss_Generator: 8.6273 D(x): 0.6780 D(G(z)): 0.0210 / 0.0003\n",
      "[0/25][12/782] Loss_Discriminator: 0.7931 Loss_Generator: 9.6058 D(x): 0.8312 D(G(z)): 0.3536 / 0.0001\n",
      "[0/25][13/782] Loss_Discriminator: 0.6057 Loss_Generator: 9.2137 D(x): 0.7598 D(G(z)): 0.1791 / 0.0002\n",
      "[0/25][14/782] Loss_Discriminator: 0.5652 Loss_Generator: 11.2704 D(x): 0.8960 D(G(z)): 0.2900 / 0.0000\n",
      "[0/25][15/782] Loss_Discriminator: 0.3142 Loss_Generator: 8.6894 D(x): 0.8322 D(G(z)): 0.0492 / 0.0003\n",
      "[0/25][16/782] Loss_Discriminator: 1.0746 Loss_Generator: 14.1891 D(x): 0.9224 D(G(z)): 0.5755 / 0.0000\n",
      "[0/25][17/782] Loss_Discriminator: 0.2856 Loss_Generator: 12.0203 D(x): 0.8083 D(G(z)): 0.0057 / 0.0000\n",
      "[0/25][18/782] Loss_Discriminator: 0.3757 Loss_Generator: 7.2271 D(x): 0.8399 D(G(z)): 0.0683 / 0.0014\n",
      "[0/25][19/782] Loss_Discriminator: 2.3712 Loss_Generator: 15.2527 D(x): 0.9007 D(G(z)): 0.8454 / 0.0000\n",
      "[0/25][20/782] Loss_Discriminator: 0.6867 Loss_Generator: 14.4660 D(x): 0.6627 D(G(z)): 0.0009 / 0.0000\n",
      "[0/25][21/782] Loss_Discriminator: 0.4959 Loss_Generator: 7.5177 D(x): 0.6878 D(G(z)): 0.0103 / 0.0014\n",
      "[0/25][22/782] Loss_Discriminator: 2.2051 Loss_Generator: 14.8307 D(x): 0.9427 D(G(z)): 0.8325 / 0.0000\n",
      "[0/25][23/782] Loss_Discriminator: 0.1821 Loss_Generator: 14.6628 D(x): 0.8566 D(G(z)): 0.0013 / 0.0000\n",
      "[0/25][24/782] Loss_Discriminator: 0.2856 Loss_Generator: 9.2672 D(x): 0.8187 D(G(z)): 0.0028 / 0.0002\n",
      "[0/25][25/782] Loss_Discriminator: 0.8890 Loss_Generator: 13.0234 D(x): 0.9527 D(G(z)): 0.5013 / 0.0000\n",
      "[0/25][26/782] Loss_Discriminator: 0.1460 Loss_Generator: 11.5675 D(x): 0.8856 D(G(z)): 0.0062 / 0.0000\n",
      "[0/25][27/782] Loss_Discriminator: 0.1064 Loss_Generator: 7.4729 D(x): 0.9373 D(G(z)): 0.0293 / 0.0011\n",
      "[0/25][28/782] Loss_Discriminator: 1.4635 Loss_Generator: 16.2775 D(x): 0.8941 D(G(z)): 0.6679 / 0.0000\n",
      "[0/25][29/782] Loss_Discriminator: 0.4261 Loss_Generator: 16.0310 D(x): 0.7295 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][30/782] Loss_Discriminator: 0.1925 Loss_Generator: 11.0749 D(x): 0.8673 D(G(z)): 0.0005 / 0.0000\n",
      "[0/25][31/782] Loss_Discriminator: 0.3038 Loss_Generator: 7.4533 D(x): 0.9059 D(G(z)): 0.1229 / 0.0010\n",
      "[0/25][32/782] Loss_Discriminator: 1.6650 Loss_Generator: 18.3700 D(x): 0.9351 D(G(z)): 0.7459 / 0.0000\n",
      "[0/25][33/782] Loss_Discriminator: 0.2769 Loss_Generator: 19.4461 D(x): 0.7851 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][34/782] Loss_Discriminator: 0.2400 Loss_Generator: 15.0188 D(x): 0.8447 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][35/782] Loss_Discriminator: 0.0880 Loss_Generator: 6.6432 D(x): 0.9311 D(G(z)): 0.0088 / 0.0024\n",
      "[0/25][36/782] Loss_Discriminator: 3.1088 Loss_Generator: 18.0258 D(x): 0.9560 D(G(z)): 0.9356 / 0.0000\n",
      "[0/25][37/782] Loss_Discriminator: 0.2321 Loss_Generator: 20.5844 D(x): 0.8248 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][38/782] Loss_Discriminator: 0.2812 Loss_Generator: 17.4840 D(x): 0.7883 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][39/782] Loss_Discriminator: 0.0798 Loss_Generator: 10.5586 D(x): 0.9342 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][40/782] Loss_Discriminator: 0.2350 Loss_Generator: 7.7693 D(x): 0.9555 D(G(z)): 0.1558 / 0.0006\n",
      "[0/25][41/782] Loss_Discriminator: 0.6857 Loss_Generator: 16.2107 D(x): 0.9498 D(G(z)): 0.4350 / 0.0000\n",
      "[0/25][42/782] Loss_Discriminator: 0.0945 Loss_Generator: 17.1310 D(x): 0.9215 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][43/782] Loss_Discriminator: 0.1100 Loss_Generator: 13.3502 D(x): 0.9049 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][44/782] Loss_Discriminator: 0.1435 Loss_Generator: 7.0793 D(x): 0.8840 D(G(z)): 0.0027 / 0.0012\n",
      "[0/25][45/782] Loss_Discriminator: 0.8443 Loss_Generator: 14.4183 D(x): 0.8269 D(G(z)): 0.4184 / 0.0000\n",
      "[0/25][46/782] Loss_Discriminator: 0.1022 Loss_Generator: 15.0946 D(x): 0.9231 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][47/782] Loss_Discriminator: 0.0790 Loss_Generator: 11.4215 D(x): 0.9329 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][48/782] Loss_Discriminator: 0.0730 Loss_Generator: 6.1184 D(x): 0.9457 D(G(z)): 0.0120 / 0.0032\n",
      "[0/25][49/782] Loss_Discriminator: 1.1602 Loss_Generator: 18.1327 D(x): 0.9728 D(G(z)): 0.6250 / 0.0000\n",
      "[0/25][50/782] Loss_Discriminator: 0.2385 Loss_Generator: 20.5530 D(x): 0.8187 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][51/782] Loss_Discriminator: 0.2275 Loss_Generator: 17.9545 D(x): 0.8624 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][52/782] Loss_Discriminator: 0.1165 Loss_Generator: 12.6416 D(x): 0.8995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][53/782] Loss_Discriminator: 0.0773 Loss_Generator: 6.5193 D(x): 0.9327 D(G(z)): 0.0024 / 0.0023\n",
      "[0/25][54/782] Loss_Discriminator: 0.5683 Loss_Generator: 14.7847 D(x): 0.9817 D(G(z)): 0.3936 / 0.0000\n",
      "[0/25][55/782] Loss_Discriminator: 0.0843 Loss_Generator: 15.8179 D(x): 0.9237 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][56/782] Loss_Discriminator: 0.1169 Loss_Generator: 13.1982 D(x): 0.9055 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][57/782] Loss_Discriminator: 0.0672 Loss_Generator: 8.5415 D(x): 0.9408 D(G(z)): 0.0006 / 0.0003\n",
      "[0/25][58/782] Loss_Discriminator: 0.1233 Loss_Generator: 6.4521 D(x): 0.9670 D(G(z)): 0.0772 / 0.0023\n",
      "[0/25][59/782] Loss_Discriminator: 0.4203 Loss_Generator: 15.6666 D(x): 0.9760 D(G(z)): 0.3058 / 0.0000\n",
      "[0/25][60/782] Loss_Discriminator: 0.1604 Loss_Generator: 15.3680 D(x): 0.8813 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][61/782] Loss_Discriminator: 0.1096 Loss_Generator: 12.4029 D(x): 0.9232 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][62/782] Loss_Discriminator: 0.0832 Loss_Generator: 6.6330 D(x): 0.9385 D(G(z)): 0.0035 / 0.0020\n",
      "[0/25][63/782] Loss_Discriminator: 0.5810 Loss_Generator: 17.0482 D(x): 0.9709 D(G(z)): 0.4034 / 0.0000\n",
      "[0/25][64/782] Loss_Discriminator: 0.2279 Loss_Generator: 17.9979 D(x): 0.8281 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][65/782] Loss_Discriminator: 0.1689 Loss_Generator: 13.9839 D(x): 0.8887 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][66/782] Loss_Discriminator: 0.0268 Loss_Generator: 7.8684 D(x): 0.9753 D(G(z)): 0.0010 / 0.0005\n",
      "[0/25][67/782] Loss_Discriminator: 0.3129 Loss_Generator: 13.0175 D(x): 0.9815 D(G(z)): 0.2442 / 0.0000\n",
      "[0/25][68/782] Loss_Discriminator: 0.0810 Loss_Generator: 12.3493 D(x): 0.9282 D(G(z)): 0.0010 / 0.0000\n",
      "[0/25][69/782] Loss_Discriminator: 0.0412 Loss_Generator: 9.4042 D(x): 0.9630 D(G(z)): 0.0012 / 0.0002\n",
      "[0/25][70/782] Loss_Discriminator: 0.1006 Loss_Generator: 6.8338 D(x): 0.9609 D(G(z)): 0.0529 / 0.0016\n",
      "[0/25][71/782] Loss_Discriminator: 0.5217 Loss_Generator: 18.6058 D(x): 0.9599 D(G(z)): 0.3549 / 0.0000\n",
      "[0/25][72/782] Loss_Discriminator: 0.2786 Loss_Generator: 19.1496 D(x): 0.8539 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][73/782] Loss_Discriminator: 0.1778 Loss_Generator: 16.1720 D(x): 0.8599 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][74/782] Loss_Discriminator: 0.0596 Loss_Generator: 8.6832 D(x): 0.9496 D(G(z)): 0.0006 / 0.0004\n",
      "[0/25][75/782] Loss_Discriminator: 0.4318 Loss_Generator: 13.6397 D(x): 0.9396 D(G(z)): 0.2727 / 0.0000\n",
      "[0/25][76/782] Loss_Discriminator: 0.0460 Loss_Generator: 13.1731 D(x): 0.9589 D(G(z)): 0.0008 / 0.0000\n",
      "[0/25][77/782] Loss_Discriminator: 0.0669 Loss_Generator: 9.3131 D(x): 0.9422 D(G(z)): 0.0010 / 0.0001\n",
      "[0/25][78/782] Loss_Discriminator: 0.2260 Loss_Generator: 9.2573 D(x): 0.9371 D(G(z)): 0.1260 / 0.0002\n",
      "[0/25][79/782] Loss_Discriminator: 0.0752 Loss_Generator: 8.1536 D(x): 0.9498 D(G(z)): 0.0156 / 0.0006\n",
      "[0/25][80/782] Loss_Discriminator: 0.2553 Loss_Generator: 11.8378 D(x): 0.9537 D(G(z)): 0.1764 / 0.0000\n",
      "[0/25][81/782] Loss_Discriminator: 0.0555 Loss_Generator: 10.8223 D(x): 0.9530 D(G(z)): 0.0027 / 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][82/782] Loss_Discriminator: 0.1770 Loss_Generator: 6.1155 D(x): 0.8856 D(G(z)): 0.0194 / 0.0034\n",
      "[0/25][83/782] Loss_Discriminator: 0.9941 Loss_Generator: 19.9823 D(x): 0.9596 D(G(z)): 0.5539 / 0.0000\n",
      "[0/25][84/782] Loss_Discriminator: 0.9107 Loss_Generator: 19.8423 D(x): 0.6006 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][85/782] Loss_Discriminator: 0.2054 Loss_Generator: 15.8387 D(x): 0.8631 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][86/782] Loss_Discriminator: 0.0350 Loss_Generator: 11.2559 D(x): 0.9772 D(G(z)): 0.0001 / 0.0001\n",
      "[0/25][87/782] Loss_Discriminator: 0.0350 Loss_Generator: 5.1225 D(x): 0.9916 D(G(z)): 0.0256 / 0.0112\n",
      "[0/25][88/782] Loss_Discriminator: 1.4339 Loss_Generator: 15.1637 D(x): 0.9918 D(G(z)): 0.7105 / 0.0000\n",
      "[0/25][89/782] Loss_Discriminator: 0.9277 Loss_Generator: 13.8700 D(x): 0.5732 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][90/782] Loss_Discriminator: 0.2355 Loss_Generator: 9.7309 D(x): 0.8879 D(G(z)): 0.0015 / 0.0005\n",
      "[0/25][91/782] Loss_Discriminator: 0.1773 Loss_Generator: 5.1003 D(x): 0.9057 D(G(z)): 0.0386 / 0.0156\n",
      "[0/25][92/782] Loss_Discriminator: 0.9595 Loss_Generator: 10.4974 D(x): 0.9784 D(G(z)): 0.5277 / 0.0000\n",
      "[0/25][93/782] Loss_Discriminator: 0.3136 Loss_Generator: 9.5681 D(x): 0.7991 D(G(z)): 0.0028 / 0.0001\n",
      "[0/25][94/782] Loss_Discriminator: 0.3363 Loss_Generator: 5.9472 D(x): 0.7899 D(G(z)): 0.0069 / 0.0042\n",
      "[0/25][95/782] Loss_Discriminator: 0.2653 Loss_Generator: 6.2290 D(x): 0.9555 D(G(z)): 0.1764 / 0.0106\n",
      "[0/25][96/782] Loss_Discriminator: 0.2808 Loss_Generator: 9.2215 D(x): 0.9772 D(G(z)): 0.2088 / 0.0003\n",
      "[0/25][97/782] Loss_Discriminator: 0.3816 Loss_Generator: 6.1628 D(x): 0.7742 D(G(z)): 0.0129 / 0.0039\n",
      "[0/25][98/782] Loss_Discriminator: 0.2313 Loss_Generator: 5.2683 D(x): 0.9335 D(G(z)): 0.1156 / 0.0064\n",
      "[0/25][99/782] Loss_Discriminator: 0.3676 Loss_Generator: 9.2960 D(x): 0.9537 D(G(z)): 0.2485 / 0.0003\n",
      "[0/25][100/782] Loss_Discriminator: 0.3906 Loss_Generator: 6.6871 D(x): 0.7632 D(G(z)): 0.0064 / 0.0053\n",
      "[0/25][101/782] Loss_Discriminator: 0.2888 Loss_Generator: 4.4549 D(x): 0.8706 D(G(z)): 0.0855 / 0.0197\n",
      "[0/25][102/782] Loss_Discriminator: 0.6949 Loss_Generator: 10.1987 D(x): 0.9735 D(G(z)): 0.4424 / 0.0001\n",
      "[0/25][103/782] Loss_Discriminator: 0.5324 Loss_Generator: 8.1343 D(x): 0.6949 D(G(z)): 0.0028 / 0.0007\n",
      "[0/25][104/782] Loss_Discriminator: 0.1920 Loss_Generator: 4.6224 D(x): 0.8726 D(G(z)): 0.0196 / 0.0178\n",
      "[0/25][105/782] Loss_Discriminator: 0.5538 Loss_Generator: 7.5152 D(x): 0.9746 D(G(z)): 0.3699 / 0.0008\n",
      "[0/25][106/782] Loss_Discriminator: 0.2495 Loss_Generator: 6.4980 D(x): 0.8258 D(G(z)): 0.0127 / 0.0019\n",
      "[0/25][107/782] Loss_Discriminator: 0.1555 Loss_Generator: 4.2876 D(x): 0.9062 D(G(z)): 0.0394 / 0.0168\n",
      "[0/25][108/782] Loss_Discriminator: 0.4605 Loss_Generator: 7.9216 D(x): 0.9682 D(G(z)): 0.3252 / 0.0020\n",
      "[0/25][109/782] Loss_Discriminator: 0.4030 Loss_Generator: 6.7753 D(x): 0.7961 D(G(z)): 0.0249 / 0.0070\n",
      "[0/25][110/782] Loss_Discriminator: 0.4089 Loss_Generator: 4.3186 D(x): 0.8181 D(G(z)): 0.0825 / 0.0231\n",
      "[0/25][111/782] Loss_Discriminator: 0.5157 Loss_Generator: 9.2621 D(x): 0.9580 D(G(z)): 0.3267 / 0.0002\n",
      "[0/25][112/782] Loss_Discriminator: 0.6949 Loss_Generator: 5.5213 D(x): 0.6089 D(G(z)): 0.0037 / 0.0183\n",
      "[0/25][113/782] Loss_Discriminator: 0.3255 Loss_Generator: 6.9086 D(x): 0.9605 D(G(z)): 0.2099 / 0.0028\n",
      "[0/25][114/782] Loss_Discriminator: 0.1251 Loss_Generator: 6.2154 D(x): 0.9457 D(G(z)): 0.0476 / 0.0032\n",
      "[0/25][115/782] Loss_Discriminator: 0.2404 Loss_Generator: 8.2385 D(x): 0.9726 D(G(z)): 0.1689 / 0.0009\n",
      "[0/25][116/782] Loss_Discriminator: 0.2336 Loss_Generator: 6.1416 D(x): 0.8526 D(G(z)): 0.0112 / 0.0100\n",
      "[0/25][117/782] Loss_Discriminator: 0.2034 Loss_Generator: 6.4272 D(x): 0.9522 D(G(z)): 0.1196 / 0.0032\n",
      "[0/25][118/782] Loss_Discriminator: 0.3507 Loss_Generator: 3.4025 D(x): 0.8101 D(G(z)): 0.0720 / 0.0467\n",
      "[0/25][119/782] Loss_Discriminator: 0.6250 Loss_Generator: 10.9497 D(x): 0.9832 D(G(z)): 0.4270 / 0.0000\n",
      "[0/25][120/782] Loss_Discriminator: 1.2903 Loss_Generator: 7.3524 D(x): 0.4613 D(G(z)): 0.0003 / 0.0133\n",
      "[0/25][121/782] Loss_Discriminator: 0.2134 Loss_Generator: 5.9644 D(x): 0.9809 D(G(z)): 0.1349 / 0.0079\n",
      "[0/25][122/782] Loss_Discriminator: 0.2097 Loss_Generator: 5.3988 D(x): 0.9288 D(G(z)): 0.0905 / 0.0100\n",
      "[0/25][123/782] Loss_Discriminator: 0.5493 Loss_Generator: 7.9294 D(x): 0.9355 D(G(z)): 0.2902 / 0.0012\n",
      "[0/25][124/782] Loss_Discriminator: 0.3240 Loss_Generator: 6.1856 D(x): 0.7952 D(G(z)): 0.0191 / 0.0048\n",
      "[0/25][125/782] Loss_Discriminator: 0.2822 Loss_Generator: 4.0886 D(x): 0.8723 D(G(z)): 0.0913 / 0.0276\n",
      "[0/25][126/782] Loss_Discriminator: 0.4471 Loss_Generator: 8.3971 D(x): 0.9568 D(G(z)): 0.2867 / 0.0005\n",
      "[0/25][127/782] Loss_Discriminator: 0.6496 Loss_Generator: 4.2708 D(x): 0.6490 D(G(z)): 0.0069 / 0.0369\n",
      "[0/25][128/782] Loss_Discriminator: 0.5187 Loss_Generator: 7.6209 D(x): 0.9267 D(G(z)): 0.3104 / 0.0010\n",
      "[0/25][129/782] Loss_Discriminator: 0.2531 Loss_Generator: 7.2956 D(x): 0.8961 D(G(z)): 0.0900 / 0.0012\n",
      "[0/25][130/782] Loss_Discriminator: 0.2098 Loss_Generator: 5.2191 D(x): 0.8739 D(G(z)): 0.0323 / 0.0128\n",
      "[0/25][131/782] Loss_Discriminator: 0.5881 Loss_Generator: 8.6771 D(x): 0.9152 D(G(z)): 0.3343 / 0.0003\n",
      "[0/25][132/782] Loss_Discriminator: 0.4922 Loss_Generator: 3.9529 D(x): 0.7000 D(G(z)): 0.0138 / 0.0493\n",
      "[0/25][133/782] Loss_Discriminator: 0.9976 Loss_Generator: 12.7374 D(x): 0.9402 D(G(z)): 0.5229 / 0.0000\n",
      "[0/25][134/782] Loss_Discriminator: 2.9118 Loss_Generator: 6.7407 D(x): 0.1819 D(G(z)): 0.0003 / 0.0091\n",
      "[0/25][135/782] Loss_Discriminator: 0.4046 Loss_Generator: 5.4030 D(x): 0.8858 D(G(z)): 0.1696 / 0.0250\n",
      "[0/25][136/782] Loss_Discriminator: 0.5258 Loss_Generator: 7.5680 D(x): 0.9194 D(G(z)): 0.2701 / 0.0012\n",
      "[0/25][137/782] Loss_Discriminator: 1.1167 Loss_Generator: 1.7657 D(x): 0.5674 D(G(z)): 0.0543 / 0.2411\n",
      "[0/25][138/782] Loss_Discriminator: 2.4778 Loss_Generator: 11.8902 D(x): 0.9961 D(G(z)): 0.8615 / 0.0001\n",
      "[0/25][139/782] Loss_Discriminator: 2.7985 Loss_Generator: 8.2690 D(x): 0.3124 D(G(z)): 0.0051 / 0.0036\n",
      "[0/25][140/782] Loss_Discriminator: 0.7938 Loss_Generator: 2.4814 D(x): 0.6507 D(G(z)): 0.0301 / 0.2399\n",
      "[0/25][141/782] Loss_Discriminator: 2.5355 Loss_Generator: 6.2282 D(x): 0.9738 D(G(z)): 0.7543 / 0.0060\n",
      "[0/25][142/782] Loss_Discriminator: 0.4373 Loss_Generator: 6.6248 D(x): 0.8198 D(G(z)): 0.1500 / 0.0044\n",
      "[0/25][143/782] Loss_Discriminator: 0.5335 Loss_Generator: 4.6293 D(x): 0.7165 D(G(z)): 0.0690 / 0.0295\n",
      "[0/25][144/782] Loss_Discriminator: 0.3838 Loss_Generator: 4.5229 D(x): 0.8992 D(G(z)): 0.2182 / 0.0170\n",
      "[0/25][145/782] Loss_Discriminator: 0.4567 Loss_Generator: 5.9503 D(x): 0.8922 D(G(z)): 0.2399 / 0.0059\n",
      "[0/25][146/782] Loss_Discriminator: 0.5519 Loss_Generator: 4.3860 D(x): 0.7345 D(G(z)): 0.1277 / 0.0271\n",
      "[0/25][147/782] Loss_Discriminator: 0.7037 Loss_Generator: 5.1455 D(x): 0.7978 D(G(z)): 0.2472 / 0.0136\n",
      "[0/25][148/782] Loss_Discriminator: 0.6895 Loss_Generator: 4.1085 D(x): 0.7174 D(G(z)): 0.1842 / 0.0282\n",
      "[0/25][149/782] Loss_Discriminator: 0.8216 Loss_Generator: 8.4438 D(x): 0.9045 D(G(z)): 0.4153 / 0.0010\n",
      "[0/25][150/782] Loss_Discriminator: 1.3720 Loss_Generator: 2.9355 D(x): 0.4189 D(G(z)): 0.0536 / 0.1595\n",
      "[0/25][151/782] Loss_Discriminator: 1.0982 Loss_Generator: 6.3010 D(x): 0.9167 D(G(z)): 0.5166 / 0.0065\n",
      "[0/25][152/782] Loss_Discriminator: 0.6919 Loss_Generator: 5.0173 D(x): 0.7031 D(G(z)): 0.1317 / 0.0144\n",
      "[0/25][153/782] Loss_Discriminator: 1.0442 Loss_Generator: 6.7257 D(x): 0.8107 D(G(z)): 0.4373 / 0.0053\n",
      "[0/25][154/782] Loss_Discriminator: 1.1367 Loss_Generator: 3.4865 D(x): 0.5538 D(G(z)): 0.0792 / 0.0593\n",
      "[0/25][155/782] Loss_Discriminator: 1.0972 Loss_Generator: 6.5189 D(x): 0.8521 D(G(z)): 0.4877 / 0.0044\n",
      "[0/25][156/782] Loss_Discriminator: 0.9915 Loss_Generator: 3.3380 D(x): 0.5667 D(G(z)): 0.1115 / 0.0548\n",
      "[0/25][157/782] Loss_Discriminator: 0.8394 Loss_Generator: 7.0064 D(x): 0.9266 D(G(z)): 0.4737 / 0.0034\n",
      "[0/25][158/782] Loss_Discriminator: 0.5742 Loss_Generator: 5.3177 D(x): 0.6673 D(G(z)): 0.0408 / 0.0164\n",
      "[0/25][159/782] Loss_Discriminator: 0.4595 Loss_Generator: 2.8813 D(x): 0.7806 D(G(z)): 0.1287 / 0.0800\n",
      "[0/25][160/782] Loss_Discriminator: 1.0256 Loss_Generator: 6.8871 D(x): 0.9136 D(G(z)): 0.5271 / 0.0027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][161/782] Loss_Discriminator: 0.7112 Loss_Generator: 4.2318 D(x): 0.5936 D(G(z)): 0.0286 / 0.0207\n",
      "[0/25][162/782] Loss_Discriminator: 0.5797 Loss_Generator: 4.9102 D(x): 0.8734 D(G(z)): 0.3239 / 0.0128\n",
      "[0/25][163/782] Loss_Discriminator: 0.7331 Loss_Generator: 6.1861 D(x): 0.8023 D(G(z)): 0.3100 / 0.0050\n",
      "[0/25][164/782] Loss_Discriminator: 0.9685 Loss_Generator: 2.1312 D(x): 0.5305 D(G(z)): 0.0603 / 0.1661\n",
      "[0/25][165/782] Loss_Discriminator: 2.1062 Loss_Generator: 8.9309 D(x): 0.9290 D(G(z)): 0.8024 / 0.0005\n",
      "[0/25][166/782] Loss_Discriminator: 1.4916 Loss_Generator: 6.7571 D(x): 0.4016 D(G(z)): 0.0095 / 0.0090\n",
      "[0/25][167/782] Loss_Discriminator: 0.5314 Loss_Generator: 3.3582 D(x): 0.7148 D(G(z)): 0.0543 / 0.0930\n",
      "[0/25][168/782] Loss_Discriminator: 0.9365 Loss_Generator: 5.5269 D(x): 0.9569 D(G(z)): 0.4592 / 0.0147\n",
      "[0/25][169/782] Loss_Discriminator: 0.4258 Loss_Generator: 5.3197 D(x): 0.8146 D(G(z)): 0.1329 / 0.0085\n",
      "[0/25][170/782] Loss_Discriminator: 0.3295 Loss_Generator: 4.1057 D(x): 0.8277 D(G(z)): 0.0967 / 0.0349\n",
      "[0/25][171/782] Loss_Discriminator: 0.7422 Loss_Generator: 6.9188 D(x): 0.8777 D(G(z)): 0.3694 / 0.0018\n",
      "[0/25][172/782] Loss_Discriminator: 0.8660 Loss_Generator: 2.8612 D(x): 0.5292 D(G(z)): 0.0180 / 0.0831\n",
      "[0/25][173/782] Loss_Discriminator: 0.9994 Loss_Generator: 6.5156 D(x): 0.9342 D(G(z)): 0.5411 / 0.0024\n",
      "[0/25][174/782] Loss_Discriminator: 0.4762 Loss_Generator: 5.0025 D(x): 0.6987 D(G(z)): 0.0280 / 0.0099\n",
      "[0/25][175/782] Loss_Discriminator: 0.2951 Loss_Generator: 3.6694 D(x): 0.8726 D(G(z)): 0.1171 / 0.0305\n",
      "[0/25][176/782] Loss_Discriminator: 0.6712 Loss_Generator: 6.3170 D(x): 0.8997 D(G(z)): 0.3843 / 0.0029\n",
      "[0/25][177/782] Loss_Discriminator: 0.4520 Loss_Generator: 4.3434 D(x): 0.7144 D(G(z)): 0.0314 / 0.0209\n",
      "[0/25][178/782] Loss_Discriminator: 0.3268 Loss_Generator: 5.5897 D(x): 0.9338 D(G(z)): 0.2105 / 0.0055\n",
      "[0/25][179/782] Loss_Discriminator: 0.3147 Loss_Generator: 6.2596 D(x): 0.8720 D(G(z)): 0.1444 / 0.0031\n",
      "[0/25][180/782] Loss_Discriminator: 0.2794 Loss_Generator: 4.9607 D(x): 0.8410 D(G(z)): 0.0633 / 0.0120\n",
      "[0/25][181/782] Loss_Discriminator: 0.5172 Loss_Generator: 4.9935 D(x): 0.7852 D(G(z)): 0.1911 / 0.0098\n",
      "[0/25][182/782] Loss_Discriminator: 0.2933 Loss_Generator: 6.7792 D(x): 0.9415 D(G(z)): 0.1883 / 0.0018\n",
      "[0/25][183/782] Loss_Discriminator: 0.1957 Loss_Generator: 6.1736 D(x): 0.9020 D(G(z)): 0.0722 / 0.0035\n",
      "[0/25][184/782] Loss_Discriminator: 0.3011 Loss_Generator: 6.4931 D(x): 0.9034 D(G(z)): 0.1597 / 0.0021\n",
      "[0/25][185/782] Loss_Discriminator: 0.7173 Loss_Generator: 4.1295 D(x): 0.6461 D(G(z)): 0.1358 / 0.0272\n",
      "[0/25][186/782] Loss_Discriminator: 0.7917 Loss_Generator: 11.1998 D(x): 0.9450 D(G(z)): 0.4366 / 0.0000\n",
      "[0/25][187/782] Loss_Discriminator: 0.7698 Loss_Generator: 8.7195 D(x): 0.5600 D(G(z)): 0.0005 / 0.0012\n",
      "[0/25][188/782] Loss_Discriminator: 0.2060 Loss_Generator: 5.5554 D(x): 0.8896 D(G(z)): 0.0343 / 0.0227\n",
      "[0/25][189/782] Loss_Discriminator: 0.4941 Loss_Generator: 10.1781 D(x): 0.9460 D(G(z)): 0.3004 / 0.0001\n",
      "[0/25][190/782] Loss_Discriminator: 0.1784 Loss_Generator: 8.0758 D(x): 0.8709 D(G(z)): 0.0116 / 0.0021\n",
      "[0/25][191/782] Loss_Discriminator: 0.1295 Loss_Generator: 6.4097 D(x): 0.9417 D(G(z)): 0.0568 / 0.0060\n",
      "[0/25][192/782] Loss_Discriminator: 0.4463 Loss_Generator: 8.5114 D(x): 0.9111 D(G(z)): 0.2254 / 0.0005\n",
      "[0/25][193/782] Loss_Discriminator: 0.1594 Loss_Generator: 7.0915 D(x): 0.8940 D(G(z)): 0.0295 / 0.0060\n",
      "[0/25][194/782] Loss_Discriminator: 0.4128 Loss_Generator: 7.3234 D(x): 0.8551 D(G(z)): 0.1610 / 0.0015\n",
      "[0/25][195/782] Loss_Discriminator: 0.2099 Loss_Generator: 6.0264 D(x): 0.8875 D(G(z)): 0.0661 / 0.0046\n",
      "[0/25][196/782] Loss_Discriminator: 0.6288 Loss_Generator: 10.5538 D(x): 0.8686 D(G(z)): 0.2663 / 0.0001\n",
      "[0/25][197/782] Loss_Discriminator: 0.8651 Loss_Generator: 5.1609 D(x): 0.6502 D(G(z)): 0.0073 / 0.0159\n",
      "[0/25][198/782] Loss_Discriminator: 0.9248 Loss_Generator: 12.4128 D(x): 0.9697 D(G(z)): 0.5044 / 0.0000\n",
      "[0/25][199/782] Loss_Discriminator: 0.3402 Loss_Generator: 10.8878 D(x): 0.7619 D(G(z)): 0.0010 / 0.0001\n",
      "[0/25][200/782] Loss_Discriminator: 0.0802 Loss_Generator: 7.2286 D(x): 0.9417 D(G(z)): 0.0131 / 0.0039\n",
      "[0/25][201/782] Loss_Discriminator: 0.8674 Loss_Generator: 11.1380 D(x): 0.9147 D(G(z)): 0.4144 / 0.0000\n",
      "[0/25][202/782] Loss_Discriminator: 0.3511 Loss_Generator: 9.3263 D(x): 0.7812 D(G(z)): 0.0058 / 0.0004\n",
      "[0/25][203/782] Loss_Discriminator: 0.1806 Loss_Generator: 5.8228 D(x): 0.9003 D(G(z)): 0.0389 / 0.0068\n",
      "[0/25][204/782] Loss_Discriminator: 0.4933 Loss_Generator: 10.5171 D(x): 0.9757 D(G(z)): 0.3334 / 0.0000\n",
      "[0/25][205/782] Loss_Discriminator: 0.3442 Loss_Generator: 9.7801 D(x): 0.8038 D(G(z)): 0.0018 / 0.0002\n",
      "[0/25][206/782] Loss_Discriminator: 0.0882 Loss_Generator: 6.8620 D(x): 0.9289 D(G(z)): 0.0036 / 0.0022\n",
      "[0/25][207/782] Loss_Discriminator: 0.2749 Loss_Generator: 4.8106 D(x): 0.9049 D(G(z)): 0.0897 / 0.0113\n",
      "[0/25][208/782] Loss_Discriminator: 0.4617 Loss_Generator: 8.9986 D(x): 0.9250 D(G(z)): 0.2705 / 0.0002\n",
      "[0/25][209/782] Loss_Discriminator: 0.2118 Loss_Generator: 7.5850 D(x): 0.8801 D(G(z)): 0.0094 / 0.0011\n",
      "[0/25][210/782] Loss_Discriminator: 0.1155 Loss_Generator: 5.7432 D(x): 0.9475 D(G(z)): 0.0406 / 0.0057\n",
      "[0/25][211/782] Loss_Discriminator: 0.6504 Loss_Generator: 11.1218 D(x): 0.9158 D(G(z)): 0.3682 / 0.0000\n",
      "[0/25][212/782] Loss_Discriminator: 0.9994 Loss_Generator: 7.3674 D(x): 0.5210 D(G(z)): 0.0016 / 0.0029\n",
      "[0/25][213/782] Loss_Discriminator: 0.3930 Loss_Generator: 6.1565 D(x): 0.8874 D(G(z)): 0.1578 / 0.0060\n",
      "[0/25][214/782] Loss_Discriminator: 0.8608 Loss_Generator: 13.2304 D(x): 0.9389 D(G(z)): 0.4249 / 0.0000\n",
      "[0/25][215/782] Loss_Discriminator: 0.9015 Loss_Generator: 10.4712 D(x): 0.5675 D(G(z)): 0.0003 / 0.0001\n",
      "[0/25][216/782] Loss_Discriminator: 0.1833 Loss_Generator: 6.0428 D(x): 0.9100 D(G(z)): 0.0052 / 0.0071\n",
      "[0/25][217/782] Loss_Discriminator: 0.4865 Loss_Generator: 10.2399 D(x): 0.9836 D(G(z)): 0.3331 / 0.0001\n",
      "[0/25][218/782] Loss_Discriminator: 0.2639 Loss_Generator: 8.4902 D(x): 0.8152 D(G(z)): 0.0064 / 0.0007\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # train with real\n",
    "        net_discriminator.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            real_cpu = real_cpu.cuda()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        label.resize_(batch_size).fill_(real_label)\n",
    "        inputv = Variable(input)\n",
    "        labelv = Variable(label)\n",
    "\n",
    "        output = net_discriminator(inputv)\n",
    "        err_discriminator_real = criterion(output, labelv)\n",
    "        err_discriminator_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = net_generator(noisev)\n",
    "        labelv = Variable(label.fill_(fake_label))\n",
    "        output = net_discriminator(fake.detach())\n",
    "        err_discriminator_fake = criterion(output, labelv)\n",
    "        err_discriminator_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        err_discriminator = err_discriminator_real + err_discriminator_fake\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        net_generator.zero_grad()\n",
    "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "        output = net_discriminator(fake)\n",
    "        err_generator = criterion(output, labelv)\n",
    "        err_generator.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_Discriminator: %.4f Loss_Generator: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 err_discriminator.data[0], err_generator.data[0], D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % outf,\n",
    "                    normalize=True)\n",
    "            fake = net_generator(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls -al output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image.open('output/real_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('output/fake_samples_epoch_024.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
